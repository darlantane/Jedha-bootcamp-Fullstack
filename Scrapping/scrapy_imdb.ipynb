{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-21 21:39:37 [scrapy.utils.log] INFO: Scrapy 2.11.1 started (bot: scrapybot)\n",
      "2025-02-21 21:39:37 [scrapy.utils.log] INFO: Versions: lxml 5.2.1.0, libxml2 2.13.1, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 23.10.0, Python 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:17:27) [MSC v.1929 64 bit (AMD64)], pyOpenSSL 24.2.1 (OpenSSL 3.0.15 3 Sep 2024), cryptography 43.0.0, Platform Windows-11-10.0.26100-SP0\n",
      "2025-02-21 21:39:37 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2025-02-21 21:39:37 [py.warnings] WARNING: c:\\Users\\darla\\anaconda3\\Lib\\site-packages\\scrapy\\utils\\request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2025-02-21 21:39:37 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n",
      "2025-02-21 21:39:37 [scrapy.extensions.telnet] INFO: Telnet Password: 7a7485496f49e9d6\n",
      "2025-02-21 21:39:37 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2025-02-21 21:39:37 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'USER_AGENT': 'Chrome/97.0'}\n",
      "2025-02-21 21:39:38 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2025-02-21 21:39:38 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2025-02-21 21:39:38 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2025-02-21 21:39:38 [scrapy.core.engine] INFO: Spider opened\n",
      "2025-02-21 21:39:38 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2025-02-21 21:39:38 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6024\n",
      "2025-02-21 21:39:38 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (308) to <GET https://www.imdb.com/chart/top/> from <GET https://www.imdb.com/chart/top>\n",
      "2025-02-21 21:39:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.imdb.com/chart/top/> (referer: None)\n",
      "2025-02-21 21:39:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://www.imdb.com/chart/top/> (referer: None)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\darla\\anaconda3\\Lib\\site-packages\\scrapy\\utils\\defer.py\", line 279, in iter_errback\n",
      "    yield next(it)\n",
      "          ^^^^^^^^\n",
      "  File \"c:\\Users\\darla\\anaconda3\\Lib\\site-packages\\scrapy\\utils\\python.py\", line 350, in __next__\n",
      "    return next(self.data)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\darla\\anaconda3\\Lib\\site-packages\\scrapy\\utils\\python.py\", line 350, in __next__\n",
      "    return next(self.data)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\darla\\anaconda3\\Lib\\site-packages\\scrapy\\core\\spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"c:\\Users\\darla\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\offsite.py\", line 28, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\darla\\anaconda3\\Lib\\site-packages\\scrapy\\core\\spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"c:\\Users\\darla\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\referer.py\", line 352, in <genexpr>\n",
      "    return (self._set_referer(r, response) for r in result or ())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\darla\\anaconda3\\Lib\\site-packages\\scrapy\\core\\spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"c:\\Users\\darla\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\urllength.py\", line 27, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, spider))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\darla\\anaconda3\\Lib\\site-packages\\scrapy\\core\\spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"c:\\Users\\darla\\anaconda3\\Lib\\site-packages\\scrapy\\spidermiddlewares\\depth.py\", line 31, in <genexpr>\n",
      "    return (r for r in result or () if self._filter(r, response, spider))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\darla\\anaconda3\\Lib\\site-packages\\scrapy\\core\\spidermw.py\", line 106, in process_sync\n",
      "    for r in iterable:\n",
      "  File \"C:\\Users\\darla\\AppData\\Local\\Temp\\ipykernel_29636\\1306584632.py\", line 9, in parse\n",
      "    for movie in response.css(\".ipc-metadata-list-summary-item\"):\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\darla\\anaconda3\\Lib\\site-packages\\scrapy\\http\\response\\text.py\", line 160, in css\n",
      "    return self.selector.css(query)\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\darla\\anaconda3\\Lib\\site-packages\\scrapy\\http\\response\\text.py\", line 145, in selector\n",
      "    self._cached_selector = Selector(self)\n",
      "                            ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\darla\\anaconda3\\Lib\\site-packages\\scrapy\\selector\\unified.py\", line 97, in __init__\n",
      "    super().__init__(text=text, type=st, **kwargs)\n",
      "  File \"c:\\Users\\darla\\anaconda3\\Lib\\site-packages\\parsel\\selector.py\", line 496, in __init__\n",
      "    root, type = _get_root_and_type_from_text(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\darla\\anaconda3\\Lib\\site-packages\\parsel\\selector.py\", line 377, in _get_root_and_type_from_text\n",
      "    root = _get_root_from_text(text, type=type, **lxml_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\darla\\anaconda3\\Lib\\site-packages\\parsel\\selector.py\", line 329, in _get_root_from_text\n",
      "    return create_root_node(text, _ctgroup[type][\"_parser\"], **lxml_kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\darla\\anaconda3\\Lib\\site-packages\\parsel\\selector.py\", line 110, in create_root_node\n",
      "    parser = parser_cls(recover=True, encoding=encoding, huge_tree=True)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\darla\\anaconda3\\Lib\\site-packages\\lxml\\html\\__init__.py\", line 1887, in __init__\n",
      "    super().__init__(**kwargs)\n",
      "  File \"src\\\\lxml\\\\parser.pxi\", line 1806, in lxml.etree.HTMLParser.__init__\n",
      "  File \"src\\\\lxml\\\\parser.pxi\", line 858, in lxml.etree._BaseParser.__init__\n",
      "LookupError: unknown encoding: 'b'utf8''\n",
      "2025-02-21 21:39:39 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2025-02-21 21:39:39 [scrapy.extensions.feedexport] INFO: Stored json feed (0 items) in: src/imdb.json\n",
      "2025-02-21 21:39:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 417,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 258732,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'downloader/response_status_count/308': 1,\n",
      " 'elapsed_time_seconds': 1.166349,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2025, 2, 21, 20, 39, 39, 643938, tzinfo=datetime.timezone.utc),\n",
      " 'httpcompression/response_bytes': 1490905,\n",
      " 'httpcompression/response_count': 1,\n",
      " 'log_count/DEBUG': 3,\n",
      " 'log_count/ERROR': 1,\n",
      " 'log_count/INFO': 11,\n",
      " 'log_count/WARNING': 1,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'spider_exceptions/LookupError': 1,\n",
      " 'start_time': datetime.datetime(2025, 2, 21, 20, 39, 38, 477589, tzinfo=datetime.timezone.utc)}\n",
      "2025-02-21 21:39:39 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "filename = \"imdb.json\"\n",
    "\n",
    "class IMDbSpider(scrapy.Spider):\n",
    "    name = \"imdb_movies\"\n",
    "    start_urls = ['https://www.imdb.com/chart/top']\n",
    "\n",
    "    def parse(self, response):\n",
    "\n",
    "        for movie in response.css(\".ipc-metadata-list-summary-item\"):\n",
    "            # Title and rank\n",
    "            title_and_rank = movie.css(\"h3.ipc-title__text::text\").get()\n",
    "            rank, title = (title_and_rank.split(\". \", 1) if title_and_rank else (None, None))\n",
    "\n",
    "            # Movie URL\n",
    "            url = movie.css(\"a.ipc-title-link-wrapper::attr(href)\").get()\n",
    "            full_url = response.urljoin(url) if url else None\n",
    "\n",
    "            # Rating\n",
    "            rating = movie.xpath('.//span[contains(@class, \"ipc-rating-star--rating\")]/text()').get()\n",
    "\n",
    "            # Votes\n",
    "            votes = movie.xpath('.//span[@class=\"ipc-rating-star--voteCount\"]//text()').getall()\n",
    "            votes = votes[1].strip('\"()')\n",
    "\n",
    "            # Yield the result\n",
    "            yield {\n",
    "                \"rank\": rank,\n",
    "                \"title\": title,\n",
    "                \"url\": full_url,\n",
    "                \"rating\": rating,\n",
    "                \"votes\": votes,\n",
    "            }\n",
    "\n",
    "\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Chrome/97.0',\n",
    "    \"FEEDS\": {\n",
    "        'src/' + filename : {\"format\": \"json\"},\n",
    "    },\n",
    "})\n",
    "\n",
    "process.crawl(IMDbSpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trailing data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc/imdb.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\darla\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:815\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\darla\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1025\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_lines(data_lines))\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1025\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1027\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mconvert_dtypes(\n\u001b[0;32m   1028\u001b[0m         infer_objects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend\n\u001b[0;32m   1029\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\darla\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1051\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m   1049\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1051\u001b[0m     obj \u001b[38;5;241m=\u001b[39m FrameParser(json, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mparse()\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\darla\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1187\u001b[0m, in \u001b[0;36mParser.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 1187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse()\n\u001b[0;32m   1189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\darla\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1403\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1399\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1402\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[1;32m-> 1403\u001b[0m         ujson_loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m     )\n\u001b[0;32m   1405\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1406\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1407\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[0;32m   1408\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ujson_loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1409\u001b[0m     }\n",
      "\u001b[1;31mValueError\u001b[0m: Trailing data"
     ]
    }
   ],
   "source": [
    "df = pd.read_json('src/imdb.json')\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
